{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bitMcCoin_1-hour.ipynb","provenance":[{"file_id":"1BDlo_4RgVclQe8XajI3A09q7bz_fnd6o","timestamp":1567732347929}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dk10wBiDbBxD","colab_type":"code","outputId":"fa248e05-6032-4e60-c765-a8244fd5d12a","executionInfo":{"status":"ok","timestamp":1574112214585,"user_tz":-120,"elapsed":581,"user":{"displayName":"Adomas Lingevicius","photoUrl":"","userId":"17902751255306312659"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","Changes compared to try7_day:\n","* instead of 24hour average bot takes 1 hour\n","* game initialization now can start with either BTC or EUR, while, in original try 7 it was only BTC\n","* crash of game removed\n","* updated reward function (tax usage)\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nChanges compared to try7_day:\\n* instead of 24hour average bot takes 8 hours\\n* game initialization now can start with either BTC or EUR, while, in original try 7 it was only BTC\\n* crash of game removed\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"hxk5wFeyljMl","colab_type":"code","outputId":"eda4c895-cec6-4e60-8262-dacd5838ea58","executionInfo":{"status":"ok","timestamp":1574466194815,"user_tz":-120,"elapsed":24459,"user":{"displayName":"Adomas Lingevicius","photoUrl":"","userId":"17902751255306312659"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jVX8HObhmljw","colab_type":"code","outputId":"cf8c78ca-3a36-45b0-e1d5-e2bcf8bb754a","executionInfo":{"status":"ok","timestamp":1574466195922,"user_tz":-120,"elapsed":2593,"user":{"displayName":"Adomas Lingevicius","photoUrl":"","userId":"17902751255306312659"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","os.chdir('/content/drive/My Drive/YouTube/BTC_bot/deepQ/')\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/YouTube/BTC_bot/deepQ'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"ldbd4-RJm6JS","colab_type":"code","outputId":"c339cbd4-5ccd-47c9-e167-1eb1f8bec151","executionInfo":{"status":"ok","timestamp":1574466199083,"user_tz":-120,"elapsed":3274,"user":{"displayName":"Adomas Lingevicius","photoUrl":"","userId":"17902751255306312659"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["'''\n","Trading training module for BTC bot. Deep Q learning version\n","'''\n","\n","# check line 31 to load/start new modelling\n","\n","from keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout\n","import keras\n","import random\n","import numpy as np\n","import pandas as pd\n","from operator import add\n","\n","\n","class TraderMan(object):\n","    init_BTC_wallet = 1  # BTC wallet in BTC\n","    init_EUR_wallet = 0  # EUR wallet in BTC\n","    \n","    init_trade_tax = 0.005  # tax ratio =0.5percent\n","    \n","    def __init__(self, BTC=init_BTC_wallet, EUR=init_EUR_wallet, trade_tax=init_trade_tax):\n","        self.reward = 0\n","        self.gamma = 0.9\n","        self.dataframe = pd.DataFrame()\n","        self.short_memory = np.array([])\n","        self.agent_target = 1\n","        self.agent_predict = 0\n","        self.learning_rate = 0.0005\n","        # self.model = self.network()\n","        self.model = self.network(\"bitMcCoin_1-hour.hdf5\")\n","        self.epsilon = 0\n","        self.actual = []\n","        self.memory = []\n","        \n","        self.BTC = BTC\n","        self.EUR = EUR\n","        self.trade_tax = trade_tax\n","        \n","        self.close_price = 1  # sets a variable to remember last closing price\n","        self.nextClosePrice = 1  # sets a variable for the upcoming price to evaluate current decision, at try5 made to be an average of a time period\n","        \n","        self.steps_done = 0 # calcs different steps taken\n","        self.BUY = 0\n","        self.SELL = 0 \n","        \n","        self.old_position = [1, 0]\n","        \n","    def last_action(self):\n","        '''\n","        Returns last action as sell or buy in binary (0,1).\n","        '''\n","        if self.EUR==0:\n","            return 1  # BUY\n","        else:\n","            return 0  # SELL\n","        \n","    def get_state(self, df, line_no):\n","        '''\n","        Gets input values\n","        '''\n","        state = df.drop(columns='Close').iloc[line_no].to_list()\n","        trade_pos = self.last_action()  # current state of the wallet\n","        state.append(trade_pos)\n","\n","        return np.asarray(state)\n","    \n","    def get_reward(self, in_coins=True, next_interval=False, raw=False, old_position=False):\n","        '''\n","        Returns wallet information. BTC and EUR\n","        \n","        in_coins - return wallet amount in BTC or EUR. Default BTC.\n","        next_interval - return wallet for next close price\n","        raw - return wallet in BTC, EUR or just one. see in_coins\n","        old_position - should the price be retuned on position before the trade or after\n","        '''\n","        # determines the price and wallet needed\n","        if next_interval==False:\n","          priceUsed = self.close_price\n","        else:\n","          priceUsed = self.nextClosePrice\n","          \n","        if old_position == False:  # checks for what to calc the prices, old position or new\n","          used_BTC = self.BTC\n","          used_EUR = self.EUR\n","        else:\n","          used_BTC = self.old_position[0]\n","          used_EUR = self.old_position[1]\n","        \n","        # returns wallet info\n","        if raw == True:\n","          return used_BTC, used_EUR\n","        else:\n","          if in_coins == True:\n","              return used_BTC, (used_EUR*(1/priceUsed))\n","          else:\n","              return (used_BTC*priceUsed), used_EUR\n","\n","    def set_reward(self, trader, crash, action):\n","        ''' \n","        Returns positive rewards for increasement of total portfolio, \n","        evaluated based on the next interval price.\n","        '''\n","        self.reward = 0\n","        \n","        # sets the score even if the action can't be processed due to lack of EUR/BTC in wallets\n","        if np.array_equal(action, [1, 0, 0]):  # BUY\n","          old_score = self.close_price\n","          new_score = self.nextClosePrice * (1-self.trade_tax) \n","        elif np.array_equal(action, [0, 0, 1]):  # SELL\n","          new_score = self.close_price * (1-self.trade_tax) \n","          old_score = self.nextClosePrice \n","        else: \n","          new_score = 1\n","          old_score = 1\n","        \n","        delta_score = (new_score - old_score)/old_score*100\n","    \n","        if delta_score != 0:\n","            self.reward = delta_score\n","\n","        return self.reward\n","        \n","    def step(self, close_price, action):\n","        '''\n","        close - closing price\n","        action - tanh output from nnet\n","        '''\n","        self.steps_done += 1\n","        \n","        a = action\n","        \n","        # readability\n","        EUR = self.EUR\n","        BTC = self.BTC\n","        trade_tax = self.trade_tax\n","        \n","        # update old price\n","        self.close_price = close_price\n","        \n","        if np.array_equal(a, [1, 0, 0]):  # BUY\n","            new_wallet = EUR*(1/close_price)\n","            new_wallet -= trade_tax*new_wallet\n","            self.BTC = new_wallet\n","            self.EUR = 0\n","            self.BUY += 1\n","        elif np.array_equal(a, [0, 0, 1]):  # SELL\n","            new_wallet = BTC*close_price\n","            new_wallet -= trade_tax*new_wallet\n","            self.EUR = new_wallet\n","            self.BTC = 0\n","            self.SELL += 1\n","        \n","    def network(self, saved_model=None):\n","        '''\n","        Builds the network.\n","        '''\n","        if saved_model:\n","            model = keras.models.load_model(saved_model)\n","        else:\n","          model = Sequential()\n","          model.add(Dense(output_dim=120, activation='relu', input_dim=16))\n","          model.add(Dropout(0.15))\n","          model.add(Dense(output_dim=120, activation='relu'))\n","          model.add(Dropout(0.15))\n","          model.add(Dense(output_dim=120, activation='relu'))\n","          model.add(Dropout(0.15))\n","          model.add(Dense(output_dim=3, activation='softmax'))\n","          opt = Adam(self.learning_rate)\n","          model.compile(loss='mse', optimizer=opt)\n","        \n","        return model\n","    \n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        \n","    def replay_new(self, memory):\n","        self.BTC = 1\n","        self.EUR = 0\n","        \n","        self.close_price = 1  # sets a variable to remember last closing price\n","        self.steps_done = 0\n","        self.BUY = 0\n","        self.SELL = 0\n","        \n","        if len(memory) > 1000:\n","            minibatch = random.sample(memory, 1000)\n","        else:\n","            minibatch = memory\n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n","            target_f = self.model.predict(np.array([state]))\n","            target_f[0][np.argmax(action)] = target\n","            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n","\n","    def train_short_memory(self, state, action, reward, next_state, done):\n","        target = reward\n","        if not done:\n","            target = reward + self.gamma * np.amax(self.model.predict(next_state.reshape((1, 16)))[0])\n","        target_f = self.model.predict(state.reshape((1, 16)))\n","        target_f[0][np.argmax(action)] = target\n","        self.model.fit(state.reshape((1, 16)), target_f, epochs=1, verbose=0)\n","    "],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"pDB0kMggnU_i","colab_type":"code","outputId":"f8653be1-62a7-4138-81d8-5b5d66de16ca","executionInfo":{"status":"ok","timestamp":1574192669457,"user_tz":-120,"elapsed":8832798,"user":{"displayName":"Adomas Lingevicius","photoUrl":"","userId":"17902751255306312659"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1EnJUquLn1O9NA-cfZ248CVe_VvnD3dN3"}},"source":["from random import randint\n","# from TraderMan import TraderMan\n","import numpy as np\n","from keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import random\n","\n","# based on\n","# https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n","# https://github.com/maurock/snake-ga\n","\n","# data load and setting\n","trainDf = pd.read_csv('./data/trainBTC2.csv')\n","trainLen = len(trainDf)\n","trainIntensity = 1440  # how many time intervals to take for samples in minutes, 10080=1 week, 1440=1day\n","\n","reward_period = (1*60)  # at which point should the nextClosePrice be taken to measure the succesess of an action, takes average of the named amount, i.e. 1440 average price for the whole day after the trade.\n","\n","lastPossibleLine = trainLen-reward_period-trainIntensity-1\n","\n","games_to_play = 500\n","\n","class Game:\n","    \n","    def __init__(self):\n","        self.score = 0\n","        self.crash = False\n","        self.steps = 0\n","        self.trader = Trader(self)\n","        \n","class Trader:\n","    \n","    def __init__(self, game):\n","        self.steps = 0\n","        self.trade_pos = 1\n","        self.last_score = 0\n","        \n","    def do_move(self, action, game, agent, closePrice, nextClosePrice):\n","        '''\n","        Checks if move is valid and if so sends it to TraderMan agent.\n","        '''\n","        self.trade_pos = agent.last_action()  # check witch wallet is active  \n","        agent.old_position = agent.get_reward(raw=True)  # updates the old position for reward calc (see set_reward())\n","        emptyAction = [0, 1, 0]  # do nothing action\n","        self.empty_move = 0 # checks if the action was empty\n","        \n","        if np.array_equal(action, [1, 0, 0]) and self.trade_pos == 1:\n","            game.crash = False\n","            # print('BUY crash')\n","            agent.step(closePrice, emptyAction)  # sends an empty step\n","            self.empty_move = 1\n","        elif np.array_equal(action, [0, 0, 1]) and self.trade_pos == 0:\n","            game.crash = False\n","            # print('SELL crash')\n","            agent.step(closePrice, emptyAction)  # send an empty step\n","            self.empty_move = 1\n","        else:\n","            agent.step(closePrice, action)\n","            self.empty_move = 0\n","            \n","        agent.nextClosePrice = nextClosePrice            \n","        game.score = sum(agent.get_reward())\n","            \n","def get_record(score, record):\n","        if score >= record:\n","            return score\n","        else:\n","            return record\n","        \n","def plot_seaborn(array_counter, array_score):\n","    sns.set(color_codes=True)\n","    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n","    ax.set(xlabel='games', ylabel='score')\n","    plt.show()\n","    \n","def initialize_game(trader, game, agent, line, df, closePrice, nextClosePrice):\n","    game.crash = False\n","    state_init1 = agent.get_state(df, line)\n","    do_rand_move = randint(0, 2)\n","    if do_rand_move == 1:\n","      action = [0, 0, 1]\n","      print('Started with EUR')\n","    else:\n","      action = [0, 1, 0]\n","      print('Started with BTC')\n","    trader.do_move(action, game, agent, closePrice, nextClosePrice)\n","    state_init2 = agent.get_state(df, line)\n","    reward1 = agent.set_reward(trader, game.crash, action)\n","    agent.remember(state_init1, action, reward1, state_init2, game.crash)\n","    agent.replay_new(agent.memory)\n","            \n","            \n","def run():\n","    agent = TraderMan()\n","    counter_games = 1\n","    score_plot = []\n","    counter_plot = []\n","    record = 0\n","    line = 0\n","    closePrice = 1\n","    nextClosePrice = 1\n","    while counter_games <= games_to_play:\n","        # Initialize classes\n","        random_steps = 0  # random steps taken\n","        game = Game()\n","        trader1 = game.trader\n","        \n","        # start gaming\n","        while not game.crash:\n","            # randomize training set selection\n","            initLine = randint(0, lastPossibleLine)\n","            endLine = initLine + trainIntensity - 1\n","            line = initLine\n","            closePrice = trainDf.Close[line]\n","            nextClosePrice = trainDf.Close[line: line+reward_period].mean()\n","            initialize_game(trader1, game, agent, line, trainDf, closePrice, nextClosePrice)\n","            \n","            plotDf = pd.DataFrame(index=range(initLine, endLine), columns=['random', 'BUY', 'SELL'], data=0)\n","            \n","            # print some trackables\n","            print('--------------------------------------')\n","            print('Game', counter_games, '\\n')\n","            print('Selected lines: ', initLine, endLine)\n","            print('Prices:         ', trainDf.Close[initLine], trainDf.Close[endLine], '\\n')\n","            \n","            # agent.epsilon is set to give randomness to actions\n","            if counter_games%10==0 or counter_games%10>7:\n","              agent.epsilon = 0\n","            else:\n","              agent.epsilon = 10 - counter_games%10\n","            # agent.epsilon = 0  # for testing\n","            \n","            # selects new price state\n","            for memb in range(initLine+1, endLine+1):\n","                #get old state\n","                state_old = agent.get_state(trainDf, line)\n","                \n","                # set up new state\n","                line = memb\n","            \n","                #perform random actions based on agent.epsilon, or choose the action\n","                if randint(0, 20) < agent.epsilon:\n","                    do_rand_move = randint(0, 2)  # should a move be done\n","                    if agent.last_action()==1 and do_rand_move==1:\n","                       final_move = [0, 0, 1]\n","                       plotDf.SELL[line] = -1\n","                    elif agent.last_action()==0 and do_rand_move==1:\n","                       final_move = [1, 0, 0]\n","                       plotDf.BUY[line] = 1\n","                    else:\n","                       final_move = [0, 0, 0]\n","        \n","                    random_steps += 1                    \n","                    plotDf.random[line] = 1\n","                else:\n","                    # predict action based on the old state\n","                    prediction = agent.model.predict(state_old.reshape((1,16)))\n","                    final_move = to_categorical(np.argmax(prediction[0]), num_classes=3)\n","                    # print('-----STATE------')\n","                    # print(state_old)\n","                    # print('-----FINAL_MOVE------')\n","                    # print(final_move)\n","                    \n","                #perform new move and get new state\n","                closePrice = trainDf.Close[line]\n","                nextClosePrice = trainDf.Close[line: line+reward_period].mean()\n","                trader1.do_move(final_move, game, agent, closePrice, nextClosePrice)\n","                if np.array_equal(final_move, [1, 0, 0]) and trader1.empty_move == 0:\n","                      plotDf.BUY[line] = 1\n","                elif np.array_equal(final_move, [0, 0, 1]) and trader1.empty_move == 0:\n","                      plotDf.SELL[line] = -1\n","                      \n","                state_new = agent.get_state(trainDf, line)\n","                \n","                #set reward for the new state\n","                reward = agent.set_reward(trader1, game.crash, final_move)\n","                \n","                # check if it's not the end of cycle\n","                if memb >= endLine:\n","                    game.crash = True\n","                \n","                #train short memory base on the new action and state\n","                agent.train_short_memory(state_old, final_move, reward, state_new, game.crash)\n","                \n","                # store the new data into a long term memory\n","                agent.remember(state_old, final_move, reward, state_new, game.crash)\n","                record = get_record(game.score, record)\n","                \n","                # move on if game crashed\n","                # if game.crash == True:\n","                #   break\n","                \n","        print(\n","              '       EUR: %.2f' %sum(agent.get_reward(in_coins=False)), '\\n',             # portfolio value at the end in EUR\n","              '      BTC: %.2f' %sum(agent.get_reward()), '\\n',                            # portfolio value at the end in BTC\n","              '      Steps done: ',  agent.steps_done,                                     # total number of steps done\n","              '%.2f' %(agent.steps_done/trainIntensity), '\\n',                             # proportion of steps taken compared to total amount of intervals\n","              '      BUYs done:  ', agent.BUY, '\\n',                                       # total number of buy steps\n","              '      SELLs done: ', agent.SELL, '\\n',                                      # total number of sell steps\n","              '      Total done:  %.2f' %((agent.SELL+agent.BUY)/(agent.steps_done+1)), '\\n',  # proportion of sum(BUY, SELL) steps compared to total amount of steps taken\n","              '      Random done: ', random_steps, '\\n'                                    # random steps taken (see epsilon)\n","             )\n","        # plot dataframe\n","        plotDf['Close'] = trainDf.Close[initLine:endLine]\n","\n","        ax1 = plotDf[['BUY', 'SELL']].plot(legend=False)  # BUY is 1, SELL -1 on the plot\n","        ax1.set_ylim([-1,1])\n","        ax2 = ax1.twinx()\n","        ax2.spines['right'].set_position(('axes', 1.0))\n","        plotDf['Close'].plot(ax=ax2, color='red', legend=False)\n","        plt.show()\n","\n","        print('--------------------------------------', '\\n')\n","        agent.replay_new(agent.memory)\n","        score_plot.append(game.score)\n","        counter_plot.append(counter_games)\n","        agent.model.save('bitMcCoin_1-hour.hdf5')\n","        if counter_games%10==0:\n","              plot_seaborn(counter_plot[-10:], score_plot[-10:])\n","        counter_games += 1\n","        plotDf.to_csv('plotDf.csv')\n","    plot_seaborn(counter_plot, score_plot)\n","\n","\n","if __name__ == '__main__':\n","    run()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}